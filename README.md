# Transformens, parameter efficient fine tuning with LoRA and Transfer Learning
During my exchange at UTS, I dove into advanced techniques to enhance Large Language Models (LLMs), aiming to make them more adaptable and efficient for specialized tasks.

This report explores the high-demand realm of model fine-tuning, covering key concepts and implementations such as:
Transformer Architecture and GPT2: Breaking down the structure that‚Äôs reshaping natural language processing.

LoRA (Low-Rank Adaptation) Fine-Tuning: A game-changer for parameter efficiency, enabling task-specific model adaptation with significantly lower computational demands.

Federated Learning with FedAvg: A decentralized, privacy-first approach allowing collaborative model training across devices, without centralizing data.

The results show impressive improvements in efficiency and performance (think ROUGE scores üìà), all while maintaining privacy and scalability. Eager to see how these innovations shape the future of AI! üåê
